{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install demucs\n",
    "!pip install soundfile\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile\n",
    "import librosa\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_path = \"/content/drive/MyDrive/songs\"\n",
    "vocals_path = \"/content/drive/MyDrive/vocals\"\n",
    "# Create directories if they do not exist\n",
    "if not os.path.exists(songs_path):\n",
    "    os.makedirs(songs_path)\n",
    "\n",
    "if not os.path.exists(vocals_path):\n",
    "    os.makedirs(vocals_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract vocals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ctr = 0 \n",
    "model = \"mdx_extra_q\"\n",
    "for audio_file in tqdm.tqdm(os.listdir(songs_path)):\n",
    "    !demucs -n mdx_extra_q --two-stems=vocals \"{os.path.join(songs_path, audio_file)}\" -o {vocals_path}\n",
    "    vocal_name=f\"vocal_{ctr}.wav\"\n",
    "    song_name = os.path.splitext(audio_file)[0]\n",
    "    \n",
    "    !mv \"{os.path.join(vocals_path, model, song_name, 'vocals.wav')}\" \"{os.path.join(vocals_path, vocal_name)}\"\n",
    "    !rm -r \"{os.path.join(vocals_path, model, song_name)}/\"\n",
    "\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from : https://github.com/openvpi/audio-slicer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_rms(\n",
    "    y,\n",
    "    *,\n",
    "    frame_length=2048,\n",
    "    hop_length=512,\n",
    "    pad_mode=\"constant\",\n",
    "):\n",
    "    padding = (int(frame_length // 2), int(frame_length // 2))\n",
    "    y = np.pad(y, padding, mode=pad_mode)\n",
    "\n",
    "    axis = -1\n",
    "    # put our new within-frame axis at the end for now\n",
    "    out_strides = y.strides + tuple([y.strides[axis]])\n",
    "    # Reduce the shape on the framing axis\n",
    "    x_shape_trimmed = list(y.shape)\n",
    "    x_shape_trimmed[axis] -= frame_length - 1\n",
    "    out_shape = tuple(x_shape_trimmed) + tuple([frame_length])\n",
    "    xw = np.lib.stride_tricks.as_strided(\n",
    "        y, shape=out_shape, strides=out_strides\n",
    "    )\n",
    "    if axis < 0:\n",
    "        target_axis = axis - 1\n",
    "    else:\n",
    "        target_axis = axis + 1\n",
    "    xw = np.moveaxis(xw, -1, target_axis)\n",
    "    # Downsample along the target axis\n",
    "    slices = [slice(None)] * xw.ndim\n",
    "    slices[axis] = slice(0, None, hop_length)\n",
    "    x = xw[tuple(slices)]\n",
    "\n",
    "    # Calculate power\n",
    "    power = np.mean(np.abs(x) ** 2, axis=-2, keepdims=True)\n",
    "\n",
    "    return np.sqrt(power)\n",
    "\n",
    "\n",
    "class Slicer:\n",
    "    def __init__(self,\n",
    "                 sr: int,\n",
    "                 threshold: float = -40.,\n",
    "                 min_length: int = 5000,\n",
    "                 min_interval: int = 300,\n",
    "                 hop_size: int = 20,\n",
    "                 max_sil_kept: int = 5000):\n",
    "        if not min_length >= min_interval >= hop_size:\n",
    "            raise ValueError('The following condition must be satisfied: min_length >= min_interval >= hop_size')\n",
    "        if not max_sil_kept >= hop_size:\n",
    "            raise ValueError('The following condition must be satisfied: max_sil_kept >= hop_size')\n",
    "        min_interval = sr * min_interval / 1000\n",
    "        self.threshold = 10 ** (threshold / 20.)\n",
    "        self.hop_size = round(sr * hop_size / 1000)\n",
    "        self.win_size = min(round(min_interval), 4 * self.hop_size)\n",
    "        self.min_length = round(sr * min_length / 1000 / self.hop_size)\n",
    "        self.min_interval = round(min_interval / self.hop_size)\n",
    "        self.max_sil_kept = round(sr * max_sil_kept / 1000 / self.hop_size)\n",
    "\n",
    "    def _apply_slice(self, waveform, begin, end):\n",
    "        if len(waveform.shape) > 1:\n",
    "            return waveform[:, begin * self.hop_size: min(waveform.shape[1], end * self.hop_size)]\n",
    "        else:\n",
    "            return waveform[begin * self.hop_size: min(waveform.shape[0], end * self.hop_size)]\n",
    "\n",
    "    def slice(self, waveform):\n",
    "        if len(waveform.shape) > 1:\n",
    "            samples = waveform.mean(axis=0)\n",
    "        else:\n",
    "            samples = waveform\n",
    "        if samples.shape[0] <= self.min_length:\n",
    "            return [waveform]\n",
    "        # if samples.shape[0]//self.hop_size <= self.min_length:\n",
    "        #     return [waveform]\n",
    "        \n",
    "        rms_list = get_rms(y=samples, frame_length=self.win_size, hop_length=self.hop_size).squeeze(0)\n",
    "        sil_tags = []\n",
    "        silence_start = None\n",
    "        clip_start = 0\n",
    "        for i, rms in enumerate(rms_list):\n",
    "            # Keep looping while frame is silent.\n",
    "            if rms < self.threshold:\n",
    "                # Record start of silent frames.\n",
    "                if silence_start is None:\n",
    "                    silence_start = i\n",
    "                continue\n",
    "            # Keep looping while frame is not silent and silence start has not been recorded.\n",
    "            if silence_start is None:\n",
    "                continue\n",
    "            # Clear recorded silence start if interval is not enough or clip is too short\n",
    "            is_leading_silence = silence_start == 0 and i > self.max_sil_kept\n",
    "            need_slice_middle = i - silence_start >= self.min_interval and i - clip_start >= self.min_length\n",
    "            if not is_leading_silence and not need_slice_middle:\n",
    "                silence_start = None\n",
    "                continue\n",
    "            # Need slicing. Record the range of silent frames to be removed.\n",
    "            if i - silence_start <= self.max_sil_kept:\n",
    "                pos = rms_list[silence_start: i + 1].argmin() + silence_start\n",
    "                if silence_start == 0:\n",
    "                    sil_tags.append((0, pos))\n",
    "                else:\n",
    "                    sil_tags.append((pos, pos))\n",
    "                clip_start = pos\n",
    "            elif i - silence_start <= self.max_sil_kept * 2:\n",
    "                pos = rms_list[i - self.max_sil_kept: silence_start + self.max_sil_kept + 1].argmin()\n",
    "                pos += i - self.max_sil_kept\n",
    "                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start\n",
    "                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept\n",
    "                if silence_start == 0:\n",
    "                    sil_tags.append((0, pos_r))\n",
    "                    clip_start = pos_r\n",
    "                else:\n",
    "                    sil_tags.append((min(pos_l, pos), max(pos_r, pos)))\n",
    "                    clip_start = max(pos_r, pos)\n",
    "            else:\n",
    "                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start\n",
    "                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept\n",
    "                if silence_start == 0:\n",
    "                    sil_tags.append((0, pos_r))\n",
    "                else:\n",
    "                    sil_tags.append((pos_l, pos_r))\n",
    "                clip_start = pos_r\n",
    "            silence_start = None\n",
    "        # Deal with trailing silence.\n",
    "        total_frames = rms_list.shape[0]\n",
    "        if silence_start is not None and total_frames - silence_start >= self.min_interval:\n",
    "            silence_end = min(total_frames, silence_start + self.max_sil_kept)\n",
    "            pos = rms_list[silence_start: silence_end + 1].argmin() + silence_start\n",
    "            sil_tags.append((pos, total_frames + 1))\n",
    "        # Apply and return slices.\n",
    "        if len(sil_tags) == 0:\n",
    "            return [waveform]\n",
    "        else:\n",
    "            chunks = []\n",
    "            if sil_tags[0][0] > 0:\n",
    "                chunks.append(self._apply_slice(waveform, 0, sil_tags[0][0]))\n",
    "            for i in range(len(sil_tags) - 1):\n",
    "                chunks.append(self._apply_slice(waveform, sil_tags[i][1], sil_tags[i + 1][0]))\n",
    "            if sil_tags[-1][1] < total_frames:\n",
    "                chunks.append(self._apply_slice(waveform, sil_tags[-1][1], total_frames))\n",
    "            return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split silence   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########       remove the silence from audio and split at that silence\n",
    "\n",
    "non_silence_audio = \"/content/drive/MyDrive/non_silenced/\"\n",
    "if not os.path.exists(non_silence_audio):\n",
    "    os.makedirs(non_silence_audio)\n",
    "\n",
    "for audio_path in os.listdir(vocals_path):\n",
    "    print(audio_path)\n",
    "    audio_path = os.path.join(vocals_path, audio_path)\n",
    "    if len(os.path.basename(audio_path).split(\".\")) == 1: \n",
    "      continue\n",
    "    audio_name = os.path.basename(audio_path).split(\".\")[0]\n",
    "    audio, sr = librosa.load(audio_path, sr=None, mono=False) \n",
    "    slicer = Slicer(\n",
    "        sr=sr,\n",
    "        threshold=-25,\n",
    "        min_length=7000,\n",
    "        min_interval=600,\n",
    "        hop_size=15,\n",
    "        max_sil_kept=700\n",
    "    )\n",
    "    chunks = slicer.slice(audio)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk.shape) > 1:\n",
    "            chunk = chunk.T  # Swap axes if the audio is stereo.\n",
    "\n",
    "        print(\"\\twrote: \"+os.path.join(non_silence_audio,f'{audio_name}_{i}.wav'))\n",
    "        soundfile.write(os.path.join(non_silence_audio,f'{audio_name}_{i}.wav'), chunk, sr)  # Save sliced audio files with soundfile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split to 6 seconds clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####       split audios in clips of 5 seconds for training\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set parameters\n",
    "min_length = 6     # minimum length of each segment in seconds\n",
    "\n",
    "sliced_dir = \"/content/drive/MyDrive/sliced/\"\n",
    "# create output directory if it doesn't exist\n",
    "if not os.path.exists(sliced_dir):\n",
    "    os.makedirs(sliced_dir)\n",
    "\n",
    "print(f\"slicing audios to clips of {min_length}seconds\")\n",
    "cnt = 0\n",
    "# loop through all .mp3 files in original_audio directory\n",
    "for file in tqdm(os.listdir(non_silence_audio)):\n",
    "    if file.endswith('.wav'):        \n",
    "        # load audio file\n",
    "        audio_name = file.split('.')[0]\n",
    "        audio_path = os.path.join(non_silence_audio, file)\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "\n",
    "        min_samples = int(min_length * sr)\n",
    "\n",
    "        # Split the audio into segments\n",
    "        segments = []\n",
    "        segment_start = 0\n",
    "        while segment_start + min_samples < len(audio):\n",
    "            segment_end = segment_start + min_samples\n",
    "            segments.append(audio[segment_start:segment_end])\n",
    "            segment_start = segment_end\n",
    "\n",
    "        # Write each segment to a separate file\n",
    "        for i, segment in enumerate(segments):\n",
    "            segment_path = os.path.join(sliced_dir,f'segments_{audio_name}_{cnt}.wav')\n",
    "            cnt += 1\n",
    "            soundfile.write(segment_path, segment, sr)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
